# Optional: Whether to allow users to see the test / blind scores of their
# submissions prior to publishing results. This should only be enabled
# if you're not concerned about overfitting. Defaults to false.
show_unpublished_scores: false

# Optional: If true, then the 7-day publishing "speed bump" is disabled,
# allowing successful submissions to be published without delay.
#
# This can be enabled temporarily to backfill a leaderboard with established
# results before making it public, and disabled after backfilling is complete.
#
# Enabling this for public Leaderboards is possible, making it easier
# to publish results. Please note that it makes overfitting a model to
# the blind labels easy. So if you enable this for your leaderboard, either
# trust your submitters, or pay attention to incoming submissions to recognize
# people gaming the system.
disable_publish_speed_bump: false

# Required: The configuration for reading results from the evaluator
evaluator:
  # Required: The metrics we expect to be generated. Your evaluator can return
  # several metrics, and must have at least one. If your evaluator produces metrics
  # not listed here, they will be ignored without causing problems.
  metrics:
      # Required: A unique identifier for the metric.
    - key: avg_lerc
      # Required: The name to be displayed in the UI for the metric.
      display_name: "Average LERC Score"
      description: "LERC Score averaged across all instances across all datasets."
    - key: cosmosqa_lerc
      display_name: "CosmosQA LERC Score"
      description: "LERC Score on CosmosQA instances."
    - key: mcscript_lerc
      display_name: "MCScript LERC Score"
      description: "LERC Score on MCScript instances."
    - key: narrativeqa_lerc
      display_name: "NarrativeQA LERC Score"
      description: "LERC Score on NarrativeQA instances."
    - key: socialiqa_lerc
      display_name: "SocialIQA LERC Score"
      description: "LERC Score on SocialIQA instances."

# Required: A description of the table of scores to show.
metrics_table:

  # Required: A list of columns to display.
  columns:
      # Required: Column name that is displayed on the page.
    - name: Average LERC Score
      description: LERC Score averaged across all instances across all datasets.
      renderer: "simple"
      metric_keys: ["avg_lerc"]
    - name: CosmosQA LERC Score
      description: LERC Score on CosmosQA instances.
      renderer: "simple"
      metric_keys: ["cosmosqa_lerc"]
    - name: MCScript LERC Score
      description: LERC Score on MCScript instances.
      renderer: "simple"
      metric_keys: ["mcscript_lerc"]
    - name: NarrativeQA LERC Score
      description: LERC Score on NarrativeQA instances.
      renderer: "simple"
      metric_keys: ["narrativeqa_lerc"]
    - name: SocialIQA LERC Score
      description: LERC Score on SocialIQA instances.
      renderer: "simple"
      metric_keys: ["socialiqa_lerc"]

# Required: Information that impacts the display of your leaderboard in the UI
metadata:
  # Optional: The groups your leaderboard belongs to. Valid ids are "darpa" and
  # "ai2". If you don't enter a value here, the leaderboard won't be displayed
  # anywhere in the UI.
  tag_ids:
    - ai2

  # Required: The logo for your leaderboard. It should reside in the file
  # ui/src/assets/images/leaderboard/ID/logo.svg where ID is the identifier of
  # this board. To create a logo, contact the ReViz team: reviz@allenai.org.
  logo: /assets/images/leaderboard/mocha/logo.svg

  # Required: An abbreviation identifying your leaderboard.
  #
  # Please think of an interesting name. For example, YRLDRBRD or XGQCCTvN are
  # bad names because they're not pronouncible nor memorable , while something
  # like QASC or ARC or DROP are better.
  short_name: MOCHA-QA

  # Required: The fully qualified leaderboard name.
  long_name: "MOCHA Question Answering Leaderboard"

  # Required: A paragraph describing your leaderboard. Markdown is not
  # supported in this field.
  description: >
    This leaderboard is for evaluating generative question answering (QA) systems' predictions on a variety of generative QA datasets.
    The primary evaluation metric is a metric that has been trained to do generative QA evaluation known as LERC (Learned Evaluation for Reading Comprehension).
    The core datasets that we evaluate model predictions on are the four generative QA datasets that make up the MOCHA dataset.

    Note: This leaderboard is not for evaluating QA metrics.
    Rather, it uses the LERC metric that is trained on the MOCHA dataset to evaluate generative QA predictions.

  # Required: An example question from your leaderboard. This field supports
  # markdown.
  example: |
    Context: In the meanwhile, Jane has been kidnapped by the criminal Arab and wonders what is keeping her husband from once again coming to her rescue.
    Question: Who kidnapped Jane?
    Reference: The criminal Arab.

  # Required: Instructions for getting the datasets associated with your
  # leaderboard.  This field supports markdown.
  getting_the_data: |
    You can get data [here](https://yourldrbrd.apps.allenai.org/dataset.tar.gz)

  # Required: An explanation of how scores are calculated. This field supports
  # markdown.
  scoring: |
    The scoring is uses a trained reading comprehension metric known as LERC.
    To read more about how LERC was trained, see the paper describing it [here](https://arxiv.org/abs/2010.03636).

  # Required: An explanation of what user submissions should look like. This
  # field supports markdown.
  predictions_format: |
    Your predictions should be stored in a JSONLines file where each line is a JSON object representing a prediction on QA instances.
    Each JSON line must have two keys: an `id` key storing the question ID and a `candidate` key storing the model's prediction on the question.
      ```
      {"id": "59508f50c13003582a1357f8dff5fd8e", "candidate": "It was haunted, accusations of witchcraft, sudden deaths"}
      {"id": "6a9d3d2e96448b7104401adeaf6269e2", "candidate": "Don't Henrique"}
      ...
      ```
    To see a predictions file with random predictions for the test set, see this file []().

  # Required: A freeform list of example models. Markdown is supported in this field.
  example_models: |
      Check out the models [here](https://github.com/allenai/yourldrbrd).

  # Required: Metadata about the affiliated team at AI2
  team:
    # Required: The team's name
    name: Your Team Name
    # Optional: A short paragraph describing the team.
    description:
      Project Your Team Name, is focused on identifying techniques for
      predicting good team names. You can find out more
      [here](https://allenai.org/]).
  # Required: A short description of your leaderboard's purpose. This field
  # supports markdown.
  purpose:
    To help all of us choose better team names, because **they're** very important.