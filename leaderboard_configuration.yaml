# Optional: Whether to allow users to see the test / blind scores of their
# submissions prior to publishing results. This should only be enabled
# if you're not concerned about overfitting. Defaults to false.
show_unpublished_scores: false

# Optional: If true, then the 7-day publishing "speed bump" is disabled,
# allowing successful submissions to be published without delay.
#
# This can be enabled temporarily to backfill a leaderboard with established
# results before making it public, and disabled after backfilling is complete.
#
# Enabling this for public Leaderboards is possible, making it easier
# to publish results. Please note that it makes overfitting a model to
# the blind labels easy. So if you enable this for your leaderboard, either
# trust your submitters, or pay attention to incoming submissions to recognize
# people gaming the system.
disable_publish_speed_bump: false

# Required: The configuration for reading results from the evaluator
evaluator:
  metrics:
      # Required: A unique identifier for the metric.
    - key: avg_lerc
      display_name: "Avg. LERC"
      description: "Macro-averaged LERC score across the constituent datasets."
    - key: avg_bleu1
      display_name: "Avg. BLEU-1"
      description: "Macro-averaged BLEU-1 score across the constituent datasets."
    - key: avg_meteor
      display_name: "Avg. METEOR"
      description: "Macro-averaged METEOR score across the constituent datasets."
    - key: cosmosqa_lerc
      display_name: "CosmosQA LERC"
      description: "LERC score on CosmosQA instances."
    - key: cosmosqa_bleu1
      display_name: "CosmosQA BLEU-1"
      description: "BLEU-1 score on CosmosQA instances."
    - key: cosmosqa_meteor
      display_name: "CosmosQA METEOR"
      description: "METEOR score on CosmosQA instances."
    - key: mcscript_lerc
      display_name: "MCScript LERC"
      description: "LERC score on MCScript instances."
    - key: mcscript_bleu1
      display_name: "MCScript BLEU-1"
      description: "BLEU-1 score on MCScript instances."
    - key: mcscript_meteor
      display_name: "MCScript METEOR"
      description: "METEOR score on MCScript instances."
    - key: narrativeqa_lerc
      display_name: "NarrativeQA LERC"
      description: "LERC score on NarrativeQA instances."
    - key: narrativeqa_bleu1
      display_name: "NarrativeQA BLEU-1"
      description: "BLEU-1 score on NarrativeQA instances."
    - key: narrativeqa_meteor
      display_name: "NarrativeQA METEOR"
      description: "METEOR score on NarrativeQA instances."
    - key: socialiqa_lerc
      display_name: "SocialIQA LERC"
      description: "LERC score on SocialIQA instances."
    - key: socialiqa_bleu1
      display_name: "SocialIQA BLEU-1"
      description: "BLEU-1 score on SocialIQA instances."
    - key: socialiqa_meteor
      display_name: "SocialIQA METEOR"
      description: "METEOR score on SocialIQA instances."

# Required: A description of the table of scores to show.
metrics_table:

  # Required: A list of columns to display.
  columns:
      # Required: Column name that is displayed on the page.
    - name: Avg. LERC
      description: Macro-averaged LERC score across the constituent datasets.
      renderer: "simple"
      metric_keys: ["avg_lerc"]
    - name: Avg. BLEU-1
      description: Macro-averaged BLEU-1 score across the constituent datasets.
      renderer: "simple"
      metric_keys: ["avg_bleu1"]
    - name: Avg. METEOR
      description: Macro-averaged METEOR score across the constituent datasets.
      renderer: "simple"
      metric_keys: ["avg_meteor"]
    - name: CosmosQA LERC
      description: LERC score on CosmosQA instances.
      renderer: "simple"
      metric_keys: ["cosmosqa_lerc"]
    - name: CosmosQA BLEU-1
      description: BLEU-1 score on CosmosQA instances.
      renderer: "simple"
      metric_keys: ["cosmosqa_bleu1"]
    - name: CosmosQA METEOR
      description: METEOR score on CosmosQA instances.
      renderer: "simple"
      metric_keys: ["cosmosqa_meteor"]
    - name: MCScript LERC
      description: LERC score on MCScript instances.
      renderer: "simple"
      metric_keys: ["mcscript_lerc"]
    - name: MCScript BLEU-1
      description: BLEU-1 score on MCScript instances.
      renderer: "simple"
      metric_keys: ["mcscript_bleu1"]
    - name: MCScript METEOR
      description: METEOR score on MCScript instances.
      renderer: "simple"
      metric_keys: ["mcscript_meteor"]
    - name: NarrativeQA LERC
      description: LERC score on NarrativeQA instances.
      renderer: "simple"
      metric_keys: ["narrativeqa_lerc"]
    - name: NarrativeQA BLEU-1
      description: BLEU-1 score on NarrativeQA instances.
      renderer: "simple"
      metric_keys: ["narrativeqa_bleu1"]
    - name: NarrativeQA METEOR
      description: METEOR score on NarrativeQA instances.
      renderer: "simple"
      metric_keys: ["narrativeqa_meteor"]
    - name: SocialIQA LERC
      description: LERC score on SocialIQA instances.
      renderer: "simple"
      metric_keys: ["socialiqa_lerc"]
    - name: SocialIQA BLEU-1
      description: BLEU-1 score on SocialIQA instances.
      renderer: "simple"
      metric_keys: ["socialiqa_bleu1"]
    - name: SocialIQA METEOR
      description: METEOR score on SocialIQA instances.
      renderer: "simple"
      metric_keys: ["socialiqa_meteor"]

# Required: Information that impacts the display of your leaderboard in the UI
metadata:
  # Optional: The groups your leaderboard belongs to. Valid ids are "darpa" and
  # "ai2". If you don't enter a value here, the leaderboard won't be displayed
  # anywhere in the UI.
  tag_ids:
    - ai2

  logo: /assets/images/leaderboard/mocha/logo.svg

  # Required: An abbreviation identifying your leaderboard.
  short_name: MOCHA-QA

  # Required: The fully qualified leaderboard name.
  long_name: "MOCHA Question Answering Leaderboard"

  # Required: A paragraph describing your leaderboard. Markdown is not
  # supported in this field.
  description: >
    Tired of multiple-choice and span-selection QA datasets and want to try your hand at generative QA but don't have faith in the metrics?
    Try out our generative QA leaderboard which uses a trained metric known as LERC (Learned Evaluation for Reading Comprehension)!
    The core datasets that we evaluate model predictions on are the four generative QA datasets that make up the MOCHA dataset.
    We provide validation and test data; what you train on is up to you!

  # Required: An example question from your leaderboard. This field supports
  # markdown.
  example: |
    **Context**: In the meanwhile, Jane has been kidnapped by the criminal Arab and wonders what is keeping her husband from once again coming to her rescue.

    **Question**: Who kidnapped Jane?

    **Reference**: The criminal Arab.

  # Required: Instructions for getting the datasets associated with your
  # leaderboard.  This field supports markdown.
  getting_the_data: |
    Once your model is trained, we provide a [validation set](https://github.com/anthonywchen/mocha-leaderboard/tree/master/data)
    and the [evaluation script](https://github.com/anthonywchen/mocha-leaderboard/blob/master/evaluator/evaluate.py)
    used in this leaderboard for you to evaluate your model.
    Read [this](https://github.com/anthonywchen/mocha-leaderboard/blob/master/evaluator/README.md) to learn about the structure of the data files and how to run the evaluation script.

    When you are ready, make predictions on the [test questions](https://github.com/anthonywchen/mocha-leaderboard/blob/master/data/test_questions.jsonl) and submit the predictions here.

  # Required: An explanation of how scores are calculated. This field supports
  # markdown.
  scoring: |
    Scoring is done via our [evaluation script](https://github.com/anthonywchen/mocha-leaderboard/blob/master/evaluator/evaluate.py).
    The main metric used to do scoring is a trained reading comprehension metric known as LERC.
    To read more about how LERC was trained, see our [EMNLP 2020](https://arxiv.org/abs/2010.03636) paper.
    We also score submissions using the BLEU-1 metric and the METEOR metric.
    Of the three metrics, LERC has been shown to assign scores more faithful to human judgements.

  # Required: An explanation of what user submissions should look like. This
  # field supports markdown.
  predictions_format: |
    Your predictions should be stored in a JSONLines file where each line is a JSON object representing a prediction on QA instances.
    Each JSON line must have two keys: an `id` key storing the question ID and a `candidate` key storing the model's prediction on the question.
      ```
      {"id": "59508f50c13003582a1357f8dff5fd8e", "candidate": "It was haunted, accusations of witchcraft, sudden deaths"}
      {"id": "6a9d3d2e96448b7104401adeaf6269e2", "candidate": "Don't Henrique"}
      ...
      ```
    To see a predictions file with random predictions for the test set, see [this file](https://github.com/anthonywchen/mocha-leaderboard/blob/master/data/random_test_predictions.jsonl).

  # Required: A freeform list of example models. Markdown is supported in this field.
#  example_models: |
#      Check out the models [here](https://github.com/allenai/yourldrbrd).

  # Required: Metadata about the affiliated team at AI2
#  team:
    # Required: The team's name
#    name: Your Team Name
  # Required: A short description of your leaderboard's purpose. This field
  # supports markdown.
#  purpose:
#    To help all of us choose better team names, because **they're** very important.